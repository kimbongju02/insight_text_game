{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/facebook/mbart-large-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(5.4405, grad_fn=<NllLossBackward0>), logits=tensor([[[ 5.9432e+01, -1.4621e+00,  3.7058e+01,  ...,  5.7458e+00,\n",
       "          -9.7392e-01,  1.5197e+01],\n",
       "         [ 5.9400e+01, -1.4594e+00,  3.6893e+01,  ...,  5.7393e+00,\n",
       "          -9.6837e-01,  1.5126e+01],\n",
       "         [ 3.2181e+00, -2.3179e-01,  1.5990e+01,  ...,  1.3184e-01,\n",
       "          -6.3852e-01,  5.7171e+00],\n",
       "         ...,\n",
       "         [ 1.6801e+01, -4.1569e-01,  2.4646e+01,  ..., -4.9608e-02,\n",
       "           2.0826e+00,  1.2186e+01],\n",
       "         [ 1.5266e+01, -3.2041e-01,  2.1764e+01,  ..., -3.2587e-01,\n",
       "           8.9423e-01,  1.1871e+01],\n",
       "         [ 2.5778e+01, -5.5827e-01,  3.6092e+01,  ...,  1.1263e+00,\n",
       "           2.3679e+00,  1.8636e+01]]], grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.0262, -0.0158,  0.0091,  ..., -0.0433, -0.0231,  0.0045],\n",
       "         [-0.0748,  0.8153,  0.9784,  ...,  0.3769,  0.1761,  0.4081],\n",
       "         [ 0.2738,  0.5439, -0.1133,  ..., -0.0880,  0.3053,  0.4062],\n",
       "         ...,\n",
       "         [-0.0965,  1.1861, -0.4845,  ...,  0.6294, -0.1603,  0.4664],\n",
       "         [-0.7139,  0.4268,  0.1848,  ...,  0.4491,  0.2085,  0.0550],\n",
       "         [ 0.0062, -0.0026,  0.0120,  ..., -0.0249,  0.0157, -0.0119]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ko_KR\")\n",
    "\n",
    "src_text = \"Monday has arrived, and my No Desire To Go To School-itis and Don't Wanna Get Out Of Bed-osis are both acting up, but I muster up my willpower and get dressed.\"\n",
    "tgt_text =  \"월요일이 다가왔고, '학교 가고 싶지 않아'와 '침대에서 벗어나고 싶지 않아'가 모두 행동하고 있지만, 나는 의지를 내서 옷을 입는다.\"\n",
    "\n",
    "model_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(tgt_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "model(**model_inputs, labels=labels) # forward pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['월요일이 왔습니다. 학교에 가지 않겠다는 저의 소망과 침대에서 빠져나오지 않겠다는 저의 소망은 둘 다 움직입니다. 하지만 저는 제 의지력을 모아 옷을 입습니다.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "article_hi = \"Monday has arrived, and my No Desire To Go To School-itis and Don't Wanna Get Out Of Bed-osis are both acting up, but I muster up my willpower and get dressed.\"\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "encoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "    **encoded_hi,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"ko_KR\"]\n",
    ")\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
